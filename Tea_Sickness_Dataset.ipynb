{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tea Sickness Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvPzWeXhoc1Ha0uHsmN6KT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SridharSola/Tea-Dataset/blob/main/Tea_Sickness_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtqYN2gI58sy",
        "outputId": "a3aa16b7-9842-4f56-db74-8dbd910b9d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#!ls \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Methods to read file names and corresponding labels and divide into train, test and validation sets. \n",
        "I choose to use 10-fold cross validation as the dataset is small. \n",
        "\"\"\"\n",
        "\n",
        "import os #for creating and removing directories\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def my_loader(path):\n",
        "  try:\n",
        "    with open(path, 'rb') as f:\n",
        "      img = cv2.imread(path)\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)#Image.fromarray(img)\n",
        "      return img#Image.open(f).convert('RGB')\n",
        "  except IOError:\n",
        "      print('Cannot load image ' + path)\n",
        "\n",
        "def get_class(label): #Returns the specific file names\n",
        "  classes = {\n",
        "      0: 'red leaf spot',\n",
        "      1: 'algal leaf',\n",
        "      2: 'bird eye spot',\n",
        "      3: 'gray light',\n",
        "      4: 'white spot',\n",
        "      5: 'Anthracnose',\n",
        "      6: 'brown blight',\n",
        "      7: 'healthy'\n",
        "  }\n",
        "  return classes[label]\n",
        "\n",
        "def Get(root): #returns the image names and labels of all images in the dataset\n",
        "    dir = []\n",
        "    labelList = []\n",
        "    i = 0\n",
        "    for i in range(0,8):\n",
        "      name = get_class(i)\n",
        "      file1 = root + '/' + name\n",
        "      file_list = os.listdir(file1)\n",
        "      for f in file_list:\n",
        "        dir.append(os.path.join(file1, f))\n",
        "        labelList.append(i)\n",
        "    #Shuffle the image names and labels using zip \n",
        "    temp = list(zip(dir, labelList))\n",
        "    random.shuffle(temp)\n",
        "    t1, t2 = zip(*temp)\n",
        "    imageList = list(t1)\n",
        "    labelList = list(t2)\n",
        "    return imageList, labelList\n",
        "\n",
        "class ImageList(data.Dataset):\n",
        "  #Actual Dataset class with necessary methods to pass to dataloader\n",
        "  def __init__(self, images, labels,  transform = None, loader = my_loader):\n",
        "    self.images = images\n",
        "    self.labels = labels\n",
        "    self.num_cls = 8\n",
        "    self.transform = transform\n",
        "    self.loader = loader\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    label = self.labels[index]\n",
        "    imgPath = self.images[index]\n",
        "    #print(imgPath)\n",
        "    img = self.loader(imgPath)\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    return img, label\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def show_img(self, index):\n",
        "    #Displays the image\n",
        "    image, label = self.__getitem__(index)\n",
        "    image = image.permute(1, 2, 0)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image)\n",
        "    print('Label:', label)\n",
        "\n",
        "\n",
        "def get_train_test_val_lists(images, labels, K, k):\n",
        "  \"\"\"\n",
        "  Images --> list of image names\n",
        "  K --> total number of folds\n",
        "  k --> current fold\n",
        "  Returns list of image names for val, test, and train set\n",
        "  based on kth fold which can be used to get\n",
        "  objects of ImageList class again\n",
        "  \"\"\"\n",
        "  \n",
        "  num_img_pfold = int(len(images) / K) #Splits according to the current fold\n",
        "  front_test = num_img_pfold*k\n",
        "  end_test = num_img_pfold*(k+1)\n",
        "  front_val = end_test\n",
        "\n",
        "  \n",
        "  if k == 9:\n",
        "    end = len(images) - 1\n",
        "    front_val = 0\n",
        "  end_val = front_val + num_img_pfold\n",
        "  if  k == 0:\n",
        "    train_imgList = images[end_val:]\n",
        "    train_labelList = labels[end_val:]\n",
        "  elif k == 9:\n",
        "    train_imgList = images[end_val:front_test]\n",
        "    train_labelList = labels[end_val:front_test]\n",
        "  else:\n",
        "    train_imgList = images[0:front_test] + images[end_val:]\n",
        "    train_labelList = labels[0:front_test] + labels[end_val:]\n",
        "  test_imgList = images[front_test:end_test]\n",
        "  test_labelList = labels[front_test:end_test]\n",
        "  val_imgList = images[front_val: end_val]\n",
        "  val_labelList = labels[front_val: end_val]\n",
        "  \n",
        "  print(\"Number of training images: \", len(train_imgList))\n",
        "  print(\"Number of validation images: \", len(val_imgList))\n",
        "  print(\"Number of test images: \", len(test_imgList))\n",
        "  return train_imgList, val_imgList, test_imgList, train_labelList, val_labelList, test_labelList\n"
      ],
      "metadata": {
        "id": "LRsRtSUZ6CzW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "Resnet models\n",
        "\n",
        "                          NOTE: only layers required are retained and fine-tuned.\n",
        "\n",
        "'''\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.features_conv = self.vgg.features[:36]\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out = out + residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=7, end2end=True):\n",
        "        self.inplanes = 64\n",
        "        self.end2end = end2end\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    # hook for the gradients of the activations\n",
        "    def activations_hook(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        bs = x.size(0)\n",
        "        f = x\n",
        "\n",
        "        f = self.conv1(f)\n",
        "        f = self.bn1(f)\n",
        "        f = self.relu(f)\n",
        "        f = self.maxpool(f)\n",
        "        \n",
        "        f = self.layer1(f)\n",
        "        #print('layer1: ',f.size())\n",
        "        f = self.layer2(f)\n",
        "        #print('layer2: ',f.size())\n",
        "        f = self.layer3(f)\n",
        "        feature = f.view(bs, -1)\n",
        "        #print('layer4: ',f.size())\n",
        "        f = self.layer4(f)\n",
        "        #print('layer4: ',f.size())\n",
        "\n",
        "        #hook for gradcam\n",
        "        #h = f.register_hook(self.activations_hook)\n",
        "\n",
        "        f = self.avgpool(f)\n",
        "        \n",
        "        f = f.squeeze(3).squeeze(2)\n",
        "        return f\n",
        "        #return  F.normalize(f) #f\n",
        "\n",
        "    # method for the gradient extraction\n",
        "    def get_activations_gradient(self):\n",
        "        return self.gradients\n",
        "        \n",
        "    # method for the activation exctraction\n",
        "    def get_activations(self, x):\n",
        "        return self.features_conv(x)\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_resnet18(ImageNet = False, load_weights = False, path = None,  compare = True, num_classes = 8):\n",
        "  if ImageNet == False:\n",
        "    net = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
        "  else:\n",
        "    net = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "  #Clipping last layer\n",
        "  final_in_ftrs = net.fc.in_features\n",
        "  net.fc = nn.Linear(final_in_ftrs, num_classes)\n",
        "  \n",
        "  return net\n"
      ],
      "metadata": {
        "id": "iQWgKtT5I7ea"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n"
      ],
      "metadata": {
        "id": "pmktrvyJKePZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating parser to store arguments to pass to main \n",
        "\n",
        "parser = argparse.ArgumentParser(description='Tea Sickness Dataset')\n",
        "\n",
        "# Optimization options\n",
        "parser.add_argument('--epochs', default=30, type=int, metavar='N',\n",
        "                    help='number of total epochs to run')\n",
        "parser.add_argument('--start-epoch', default=1, type=int, metavar='N',\n",
        "                    help='manual epoch number (useful on restarts)')\n",
        "parser.add_argument('--batch-size', default=96, type=int, metavar='N',\n",
        "                    help='train batchsize')\n",
        "parser.add_argument('--lr', '--learning-rate', default=0.0001, type=float,\n",
        "                    metavar='LR', help='initial learning rate')\n",
        "parser.add_argument('--workers', type=int, default=16,\n",
        "                        help='num of workers to use')\n",
        "parser.add_argument('--folds', default=10, type=int, metavar='N', help='cross validation folds')\n",
        "\n",
        "# Checkpoints\n",
        "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
        "                    help='path to latest checkpoint (default: none)')\n",
        "\n",
        "#Device options\n",
        "parser.add_argument('--gpu', default='0,1', type=str,\n",
        "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
        "\n",
        "#Method options\n",
        "parser.add_argument('--train-iteration', type=int, default=800,\n",
        "                        help='Number of iteration per epoch')\n",
        "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',  help='momentum')\n",
        "\n",
        "parser.add_argument('--weight-decay', '--wd', default=1e-3, type=float,  metavar='W', help='weight decay (default: 1e-4)')\n",
        "\n",
        "parser.add_argument('--print-freq', '-p', default=1000, type=int,metavar='N', help='print frequency (default: 10)')\n",
        "\n",
        "parser.add_argument('--imagesize', type=int, default = 224, help='image size (default: 224)')\n",
        "\n",
        "\n",
        "#Data\n",
        "parser.add_argument('--classes', type=int, default=8)\n",
        "\n",
        "parser.add_argument('--root', type=str, default='/content/drive/MyDrive/tea sickness dataset',\n",
        "                        help=\"root path to train data directory\")\n",
        "\n",
        "parser.add_argument('--model-dir', default='/content/drive/MyDrive/tea sickness dataset/Checkpoint', type=str)\n",
        "\n",
        "parser.add_argument('--logfile', default='/content/drive/MyDrive/tea sickness dataset/Tea.txt', type=str)\n",
        "\n",
        "#args = parser.parse_args()\n",
        "args = parser.parse_args(\" \".split())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #To use GPU\n",
        "\n",
        "\n",
        "best_acc = 0\n",
        "def main(args):\n",
        "    global best_acc\n",
        "\n",
        "    #Data\n",
        "    mean = [0.5, 0.5, 0.5]\n",
        "    std = [0.5, 0.5, 0.5]\n",
        "    imagesize = args.imagesize\n",
        "    train_transform = transforms.Compose([transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=0.4, contrast = 0.3, saturation = 0.25, hue = 0.05),    \n",
        "            transforms.Resize((args.imagesize, args.imagesize)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean,std)\n",
        "        ])\n",
        "\n",
        "        \n",
        "    valid_transform = transforms.Compose([transforms.ToPILImage(),\n",
        "            transforms.Resize((args.imagesize,args.imagesize)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean,std)\n",
        "        ])\n",
        "    log = open(args.logfile, 'w')\n",
        "    \n",
        "    #K-Fold Cross Validation Starts here\n",
        "    for fold in range(0, args.folds):\n",
        "      print(\"\\n***************************************************************************************\\n FOLD: \", fold)\n",
        "      print(\"\\n***************************************************************************************\\n FOLD: \", fold, file =log)\n",
        "      \n",
        "      net = load_resnet18(False) #Creating model without loading any pretrained weights\n",
        "\n",
        "      optimizer =  torch.optim.Adam([{\"params\": net.parameters(), \"lr\": args.lr, \"momentum\":args.momentum,\n",
        "                                 \"weight_decay\":args.weight_decay}])\n",
        "      lrs = []\n",
        "      lrs.append(args.lr)\n",
        "\n",
        "      if args.resume:\n",
        "          print(\"Resuming from previous fold\")\n",
        "          if os.path.isfile(args.resume):\n",
        "              print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
        "              checkpoint = torch.load(args.resume)\n",
        "              ch = checkpoint['model_state_dict']\n",
        "              \n",
        "              net.load_state_dict(ch)\n",
        "              \n",
        "              optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "              print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(args.resume, checkpoint['epoch']))\n",
        "          else:\n",
        "              print(\"=> no checkpoint found at '{}'\".format(args.resume))  \n",
        "\n",
        "      images, labels = Get(args.root)\n",
        "      train_imgs, val_imgs, test_imgs, train_lab, val_lab, test_lab = get_train_test_val_lists(images, labels, args.folds, fold) \n",
        "      train_dataset = ImageList(train_imgs, train_lab, train_transform)\n",
        "      val_dataset = ImageList(val_imgs, val_lab,  valid_transform)\n",
        "      test_dataset = ImageList(test_imgs, test_lab,  valid_transform)\n",
        "      train_loader = torch.utils.data.DataLoader(train_dataset, args.batch_size, shuffle=True,\n",
        "                                                   num_workers=args.workers, pin_memory=True)\n",
        "      val_loader = torch.utils.data.DataLoader(val_dataset, args.batch_size, shuffle=False, num_workers=8)\n",
        "      test_loader = torch.utils.data.DataLoader(test_dataset, args.batch_size, shuffle=False, num_workers=8)\n",
        "      criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "      print(\"\\nStarting Training\\n\")\n",
        "\n",
        "      for epoch in range(args.start_epoch, args.epochs):\n",
        "        if epoch == 14 or epoch == 20 or epoch == 25:\n",
        "          adjust_learning_rate(optimizer, epoch)\n",
        "          lrs.append(optimizer.param_groups[0][\"lr\"])\n",
        "          print(f'Updated lr: {lrs[-1]}\\n', file = log)\n",
        "          print(f'Updated lr: {lrs[-1]}\\n')\n",
        "        train(train_loader, net, criterion, optimizer, epoch, args.epochs, log)\n",
        "        acc = validate(val_loader, net, criterion, epoch)\n",
        "        print(\"Epoch: {}   Validation Set Acc: {:.4f}\".format(epoch, acc))\n",
        "        print(\"Epoch: {}   Validation Set Acc: {:.4f}\".format(epoch, acc), file = log)\n",
        "\n",
        "        #Save best_acc and checkpoint\n",
        "        is_best = acc > best_acc\n",
        "        best_acc = max(acc, best_acc)\n",
        "        print('\\n*********************************\\nBest accuracy so far is : ', '%.4f'%best_acc)\n",
        "        print('\\n*********************************\\nBest accuracy so far is : ', '%.4f'%best_acc, log)\n",
        "\n",
        "        save_checkpoint({'epoch': epoch+1, 'model_state_dict': net.state_dict(), \n",
        "                         'best_acc': best_acc, 'optimizer': optimizer.state_dict()},\n",
        "                        is_best, 'checkpoint.pth.tar', fold)\n",
        "      test(test_loader, net, criterion)\n",
        "      conf_mat(net, test_loader)\n",
        "def save_checkpoint(state, is_best, filename = 'checkpoint.pth.tar', fold = 0):\n",
        "  epoch_num = state['epoch']\n",
        "  full_bestname = os.path.join(args.model_dir, str(fold)+' model_best.pth.tar')\n",
        "  if is_best:\n",
        "      torch.save(state, full_bestname)\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "  for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] /= 10\n",
        "    \n",
        "def accuracy(output, target, topk=(1,)):\n",
        "  \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "  maxk = max(topk)\n",
        "  batch_size = target.size(0)\n",
        "\n",
        "  _, pred = output.topk(maxk, 1, True, True)\n",
        "  pred = pred.t()\n",
        "  correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "  res = []\n",
        "  for k in topk:\n",
        "      correct_k = correct[:k].view(-1).float().sum(0)\n",
        "      res.append(correct_k.mul_(100.0 / batch_size))\n",
        "  return res\n",
        "\n",
        "def train(train_loader, net, criterion, optimizer, epoch, n, log):\n",
        "  net.train()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total=0\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    probs = net(data)\n",
        "    loss = criterion(probs, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    _, preds = torch.max(probs, dim = 1)\n",
        "    correct += torch.sum(preds==target).item()\n",
        "    total += target.size(0)\n",
        "    acc = 100 * correct/total\n",
        "    if batch_idx%args.print_freq == 0:\n",
        "      print(\"Training Epoch: {}/{}\\tLoss: {:.4f}\\nTrain Accuracy: {:.4f}\". format(epoch, n, loss.item(), acc))\n",
        "      print('Training Epoch: {}/{}\\tLoss: {:.4f}\\nTrain Accuracy: {:.4f}' . format(epoch, n, loss.item(), acc), file = log)\n",
        "\n",
        "def validate(val_loader, net, criterion, epoch):\n",
        "  net.eval()\n",
        "  batch_loss = 0\n",
        "  total=0\n",
        "  correct=0\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(val_loader):\n",
        "      data =  data.to(device)\n",
        "      target = target.to(device)\n",
        "      probs = net(data)\n",
        "      loss = criterion(probs, target)\n",
        "      _, preds = torch.max(probs, dim = 1)\n",
        "      correct += torch.sum(preds==target).item()\n",
        "      total += target.size(0)\n",
        "  acc = 100 * correct/total\n",
        "  #print(f\"Validation Set Accuracy: {(100 * correct/total):.4f}\\n\")\n",
        "  return acc\n",
        "\n",
        "def test(test_loader, net, criterion):\n",
        "  net.eval()\n",
        "  batch_loss = 0\n",
        "  total=0\n",
        "  correct=0\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "      data =  data.to(device)\n",
        "      target = target.to(device)\n",
        "      probs = net(data)\n",
        "      loss = criterion(probs, target)\n",
        "      _, preds = torch.max(probs, dim = 1)\n",
        "      correct += torch.sum(preds==target).item()\n",
        "      total += target.size(0)\n",
        "  acc = 100 * correct/total\n",
        "  #print(f\"Validation Set Accuracy: {(100 * correct/total):.4f}\\n\")\n",
        "  print(\"Test accuracy: \", acc)\n",
        "\n",
        "def conf_mat(net, test):\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  net.eval()\n",
        "  # iterate over test data\n",
        "  for inputs, labels in test:\n",
        "          output = net(inputs) # Feed Network\n",
        "\n",
        "          output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
        "          y_pred.extend(output) # Save Prediction\n",
        "          \n",
        "          labels = labels.data.cpu().numpy()\n",
        "          y_true.extend(labels) # Save Truth\n",
        "\n",
        "  # constant for classes\n",
        "  classes = {\n",
        "      'red leaf spot',\n",
        "      'algal leaf',\n",
        "      'bird eye spot',\n",
        "      'gray light',\n",
        "      'white spot',\n",
        "      'Anthracnose',\n",
        "      'brown blight',\n",
        "      'healthy'\n",
        "  }\n",
        "\n",
        "  # Build confusion matrix\n",
        "  cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "  df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
        "                      columns = [i for i in classes])\n",
        "\n",
        "  df_conf_norm = df_cm / df_cm.sum(axis=1)\n",
        "  plt.figure(figsize = (12,7))\n",
        "  sn.heatmap(df_conf_norm, annot=True)\n",
        "  plt.savefig('output.png')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    main(args)\n",
        "    print(\"Completed K-fold Cross Validation!\")                \n",
        "                  \n",
        "                  \n",
        "                  \n"
      ],
      "metadata": {
        "id": "4Xu31TbnKfB2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}